{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "You are provided with starter code that loads the data, extracts a set of features, and then trains a Naïve Bayes classifier using those features and outputs the classifier accuracy.  Your job is to extract additional feature sets using NLTK and report the classifier performance for each set of features."
      ],
      "metadata": {
        "id": "SzO6b2bt6HPI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "DSgFvHzhSPNK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "037f602e-4042-49ba-a15d-d36c4c903fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import urllib.request\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn import preprocessing\n",
        "import string\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "non_clickbait_url = \"http://www.cs.columbia.edu/~sarahita/CL/non_clickbait_data.txt\"\n",
        "clickbait_url = \"http://www.cs.columbia.edu/~sarahita/CL/clickbait_data.txt\"\n",
        "\n",
        "# read url .txt file into string \"data\"\n",
        "def get_data(url):\n",
        "  data = urllib.request.urlopen(url).read().decode('utf-8')\n",
        "  return data\n",
        "\n",
        "non_clickbait_data = get_data(non_clickbait_url)\n",
        "clickbait_data = get_data(clickbait_url)"
      ],
      "metadata": {
        "id": "mQ8JQ1scSabd"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine clickbait and non-clickbait data in a single list\n",
        "non_clickbait_headlines = non_clickbait_data.rstrip('\\n').split('\\n')\n",
        "clickbait_headlines = clickbait_data.rstrip('\\n').split('\\n')\n",
        "all_headlines = non_clickbait_headlines + clickbait_headlines"
      ],
      "metadata": {
        "id": "Hi_kgavhSdcv"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list of corresponding labels\n",
        "non_cb_labels = [0] * len(non_clickbait_headlines)\n",
        "cb_labels = [1] * len(clickbait_headlines)\n",
        "all_labels = non_cb_labels + cb_labels"
      ],
      "metadata": {
        "id": "u4FcadFJSdXi"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1: Stop words: counts for each function word (from the NLTK stopwords list)"
      ],
      "metadata": {
        "id": "5TjY6i-gDc2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stop_words(texts):\n",
        "        bow = []\n",
        "        eng_stopwords = stopwords.words('english')\n",
        "        for text in texts:      \n",
        "                counts = []\n",
        "                tokens = nltk.word_tokenize(text.lower())\n",
        "                for sw in eng_stopwords:\n",
        "                        sw_count = tokens.count(sw)\n",
        "                        counts.append(sw_count)\n",
        "                bow.append(counts)\n",
        "        bow_np = np.array(bow).astype(float)\n",
        "        return bow_np\n",
        "\n",
        "stop_words_features = stop_words(all_headlines)\n",
        "print(stop_words_features)\n",
        "\n",
        "# convert features and labels to numpy arrays\n",
        "X = stop_words_features\n",
        "Y = np.array(all_labels)\n",
        "\n",
        "# run classifier using 10-fold cross validation\n",
        "# report mean accuracy \n",
        "\n",
        "scores = cross_val_score(MultinomialNB(), X, Y, scoring='accuracy', cv=10)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "id": "-QHb6D8zSdNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0353a83e-97b7-4380-d796-3b9492294688"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [2. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "0.8735535323538606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: Syntactic: counts for the following 10 common POS tags -- ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS','CC','PRP','VB','VBG']\n"
      ],
      "metadata": {
        "id": "aBxFsRQ_Dgyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def syntactic(texts):\n",
        "        bow = []\n",
        "        POS = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS','CC','PRP','VB','VBG']\n",
        "        for text in texts:  \n",
        "                counts = Counter({})\n",
        "                current_counts = []\n",
        "                tokens = nltk.word_tokenize(text.lower())\n",
        "                for tag in POS:\n",
        "                        tag_count = []\n",
        "                        pos = nltk.pos_tag(tokens)\n",
        "                        v = 0\n",
        "                        counts = Counter(tag for _, tag in pos)\n",
        "                        for key, value in counts.items():\n",
        "                          if key == tag:\n",
        "                            v = value\n",
        "                        current_counts.append(v)\n",
        "                bow.append(current_counts)\n",
        "        bow_np = np.array(bow).astype(float)\n",
        "        return bow_np\n",
        "\n",
        "syntactic_features = syntactic(all_headlines)\n",
        "print(syntactic_features)\n",
        "\n",
        "# convert features and labels to numpy arrays\n",
        "X = syntactic_features\n",
        "Y = np.array(all_labels)\n",
        "\n",
        "# run classifier using 10-fold cross validation\n",
        "# report mean accuracy \n",
        "\n",
        "scores = cross_val_score(MultinomialNB(), X, Y, scoring='accuracy', cv=10)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "id": "-njzxrpHNtki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eaa4f86-f299-4dd0-c498-504407e0b118"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4. 0. 0. ... 0. 1. 1.]\n",
            " [2. 0. 1. ... 0. 0. 0.]\n",
            " [3. 0. 0. ... 0. 0. 1.]\n",
            " ...\n",
            " [1. 0. 1. ... 0. 0. 2.]\n",
            " [2. 0. 1. ... 0. 0. 0.]\n",
            " [4. 0. 2. ... 0. 0. 0.]]\n",
            "0.7672044584245077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3: Lexical: counts for 30 most common unigrams in entire corpus (remove stopwords and punctuation for unigram count)"
      ],
      "metadata": {
        "id": "bz54_vvhW06x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lexical(texts):\n",
        "        eng_stopwords = stopwords.words('english')\n",
        "        punctuation = set(string.punctuation)\n",
        "        mostcommon = []\n",
        "        for text in texts:      \n",
        "                counts = []\n",
        "                tokens = nltk.word_tokenize(text.lower())\n",
        "                remove_stopwords = [word for word in tokens if word not in eng_stopwords]\n",
        "                remove_punct = [word for word in remove_stopwords if word not in punctuation]\n",
        "                mostcommon.append(remove_punct)\n",
        "        flat_lex = [item for sublist in mostcommon for item in sublist]\n",
        "        mostCommon = nltk.FreqDist(flat_lex).most_common(30)\n",
        "\n",
        "        bow = []\n",
        "        mostCommon = [x[0] for x in mostCommon]\n",
        "        for text in texts:\n",
        "              counts = []\n",
        "              tokens = nltk.word_tokenize(text.lower())\n",
        "              for wrd in mostCommon:\n",
        "                  wrd_count = tokens.count(wrd)\n",
        "                  counts.append(wrd_count)\n",
        "              bow.append(counts)\n",
        "        bow_np = np.array(bow).astype(float)\n",
        "        return bow_np\n",
        "\n",
        "lexical_features = lexical(all_headlines)\n",
        "print(lexical_features)\n",
        "\n",
        "# convert features and labels to numpy arrays\n",
        "X = lexical_features\n",
        "Y = np.array(all_labels)\n",
        "\n",
        "# run classifier using 10-fold cross validation\n",
        "# report mean accuracy \n",
        "\n",
        "scores = cross_val_score(MultinomialNB(), X, Y, scoring='accuracy', cv=10)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxYqYtyXW2hP",
        "outputId": "6b8b5ff7-a9fc-4124-9e47-c17e966f236e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "0.7349466337136605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Punctuation:  Counts for each punctuation mark in string.punctuation\n"
      ],
      "metadata": {
        "id": "w2tik7NSIYBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def punctuation(texts):\n",
        "        bow = []\n",
        "        punctuation = set(string.punctuation)\n",
        "        for text in texts:      \n",
        "                counts = []\n",
        "                tokens = nltk.word_tokenize(text.lower())\n",
        "                for punct in  punctuation:\n",
        "                        punct_count = tokens.count(punct)\n",
        "                        counts.append(punct_count)\n",
        "                bow.append(counts)\n",
        "        bow_np = np.array(bow).astype(float)\n",
        "        return bow_np\n",
        "\n",
        "punctuation_features = punctuation(all_headlines)\n",
        "print(punctuation_features)\n",
        "\n",
        "# convert features and labels to numpy arrays\n",
        "X = punctuation_features\n",
        "Y = np.array(all_labels)\n",
        "\n",
        "# run classifier using 10-fold cross validation\n",
        "# report mean accuracy \n",
        "\n",
        "scores = cross_val_score(MultinomialNB(), X, Y, scoring='accuracy', cv=10)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7gjbvwyIXN3",
        "outputId": "ff55b5dc-a2f4-4062-bbe6-70360bcfd79a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "0.5012524812441388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5: Complexity: average number of characters per word #unique words/#total words number of words Count of “long” words - words with >= 6 letters"
      ],
      "metadata": {
        "id": "6UVPB7vpSU6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def avg_char_per_word(text):\n",
        "        avgChar = []\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        for word in tokens:\n",
        "            avgChar.append(len(tokens))\n",
        "        return sum(avgChar) / len(avgChar)\n",
        "\n",
        "def unique_word_percent(text):\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        return len(set(tokens))/len(tokens)\n",
        "\n",
        "def num_words(text):\n",
        "        count = 0\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        for word in tokens:      \n",
        "              count +=1\n",
        "        return count\n",
        "\n",
        "def num_long_words(text):\n",
        "        count = 0\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        for word in tokens:      \n",
        "            if len(word) > 5:\n",
        "              count +=1\n",
        "        return count\n",
        "\n",
        "def complexity(texts):\n",
        "        bow = []\n",
        "        for text in texts:      \n",
        "                avgCharPerWord = avg_char_per_word(text)\n",
        "                uniqueWordPercent = unique_word_percent(text)\n",
        "                numWords = num_words(text)\n",
        "                numLongWords = num_long_words(text)\n",
        "                bow.append([avgCharPerWord, uniqueWordPercent, numWords, numLongWords])\n",
        "        bow_np = np.array(bow).astype(float)\n",
        "        return bow_np\n",
        "\n",
        "complexity_features = complexity(all_headlines)\n",
        "print(complexity_features)\n",
        "\n",
        "# convert features and labels to numpy arrays\n",
        "X = complexity_features\n",
        "Y = np.array(all_labels)\n",
        "\n",
        "# run classifier using 10-fold cross validation\n",
        "# report mean accuracy \n",
        "\n",
        "scores = cross_val_score(MultinomialNB(), X, Y, scoring='accuracy', cv=10)\n",
        "print(scores.mean())\n",
        "\n",
        "# 10.523035441639866\n",
        "# 0.08085551695336646\n",
        "# 307697\n",
        "# 104454"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFBl9WYySaQ5",
        "outputId": "b17a481d-59f2-4d5c-aedc-a00ecbb1d916"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[13.          1.         13.          4.        ]\n",
            " [ 8.          1.          8.          4.        ]\n",
            " [12.          1.         12.          4.        ]\n",
            " ...\n",
            " [14.          1.         14.          4.        ]\n",
            " [ 9.          0.77777778  9.          1.        ]\n",
            " [13.          1.         13.          6.        ]]\n",
            "0.7004498769146609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Your own proposed feature set: Think about what other features may be useful for clickbait identification and implement them.  You can get ideas from this paper: https://arxiv.org/pdf/1610.09786.pdf  "
      ],
      "metadata": {
        "id": "xLDcTYjzV55M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def possesive(texts):\n",
        "        bow = []\n",
        "        possesive_words = [\"of\", \"'s\",  \"my\", \"mine\", \"your\", \"yours\", \"his\", \"her\", \"hers\", \"its\", \"our\", \"ours\", \"their\", \"theirs\"]\n",
        "        for text in texts:      \n",
        "                counts = []\n",
        "                tokens = nltk.word_tokenize(text.lower())\n",
        "                for word in possesive_words:\n",
        "                        word_count = tokens.count(word)\n",
        "                        counts.append(word_count)\n",
        "                bow.append(counts)\n",
        "        bow_np = np.array(bow).astype(float)\n",
        "        return bow_np\n",
        "\n",
        "possesive_features = possesive(all_headlines)\n",
        "print(possesive_features)\n",
        "\n",
        "# convert features and labels to numpy arrays\n",
        "X = possesive_features\n",
        "Y = np.array(all_labels)\n",
        "\n",
        "# run classifier using 10-fold cross validation\n",
        "# report mean accuracy \n",
        "\n",
        "scores = cross_val_score(MultinomialNB(), X, Y, scoring='accuracy', cv=10)\n",
        "print(scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JESktdKjxVxW",
        "outputId": "dcb98a4b-d14f-422f-9f60-19ba928e343a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "0.613316807986871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine all features!"
      ],
      "metadata": {
        "id": "ie50bStexYhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combined(texts):\n",
        "        bow = []    \n",
        "        stop_words_features = stop_words(texts)\n",
        "        syntactic_features = syntactic(texts)\n",
        "        lexical_features = lexical(texts)\n",
        "        punctuation_features = punctuation(texts)\n",
        "        complexity_features = complexity(texts)\n",
        "        possesive_features = possesive(texts)\n",
        "        bow_np = np.concatenate((stop_words_features, syntactic_features, lexical_features, punctuation_features, complexity_features, possesive_features), axis=1)\n",
        "        return bow_np\n",
        "\n",
        "combined_features = combined(all_headlines)\n",
        "print(combined_features)\n",
        "\n",
        "# convert features and labels to numpy arrays\n",
        "X = combined_features\n",
        "Y = np.array(all_labels)\n",
        "\n",
        "# run classifier using 10-fold cross validation\n",
        "# report mean accuracy \n",
        "\n",
        "scores = cross_val_score(MultinomialNB(), X, Y, scoring='accuracy', cv=10)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10cAmcYxTcCm",
        "outputId": "13958271-61d0-4153-c6a6-613d0442ecfd"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [2. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "0.9185888754298219\n"
          ]
        }
      ]
    }
  ]
}