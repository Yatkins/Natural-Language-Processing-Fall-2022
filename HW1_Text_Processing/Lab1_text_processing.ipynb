{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vws20JXrnB9z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68b86aa6-cbc2-4c28-f9e2-946d88bb36da"
      },
      "source": [
        "# imports\n",
        "\n",
        "import urllib.request\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzd6_9B5nirE"
      },
      "source": [
        "# load the data\n",
        "shakespeare_url = \"http://www.cs.columbia.edu/~sarahita/CL/lab1/shakespeare.txt\"\n",
        "news_url = \"http://www.cs.columbia.edu/~sarahita/CL/lab1/news.txt\"\n",
        "swbd_url = \"http://www.cs.columbia.edu/~sarahita/CL/lab1/swbd.txt\"\n",
        "\n",
        "# read url .txt file into string \"data\"\n",
        "def get_data(url):\n",
        "  data = urllib.request.urlopen(url).read().decode('utf-8')\n",
        "  return data\n",
        "\n",
        "shakespeare_data = get_data(shakespeare_url)\n",
        "news_data = get_data(news_url)\n",
        "swbd_data = get_data(swbd_url)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjErYLrmzA1I",
        "outputId": "bf082022-c901-40bb-9819-6444901e9131"
      },
      "source": [
        "print(swbd_data[:100])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "A.1: Uh, do you have a pet Randy? \n",
            "B.2: Uh, yeah, currently we have a poodle. \n",
            "A.3: A poodle, minia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Tokenization**\n",
        "\n",
        "Tokenize each of the 3 datasets using the NLTK word tokenizer. \n",
        "Count the number of tokens for each dataset, as well as the vocabulary size (number of unique tokens).  Record the results in the table below.  (1 pt)"
      ],
      "metadata": {
        "id": "xK7JtAV3CKx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_shakespeare = word_tokenize(shakespeare_data)\n",
        "tokenized_news = word_tokenize(news_data)\n",
        "tokenized_swbd = word_tokenize(swbd_data)\n",
        "\n",
        "numTokens_shakespeare = len(tokenized_shakespeare)\n",
        "numTokens_news = len(tokenized_news)\n",
        "numTokens_swbd = len(tokenized_swbd)\n",
        "\n",
        "vocabSize_shakespeare = len(set(tokenized_shakespeare))\n",
        "vocabSize_news = len(set(tokenized_news))\n",
        "vocabSize_swbd = len(set(tokenized_swbd))\n",
        "\n",
        "vocabSize_shakespeare/numTokens_shakespeare\n",
        "vocabSize_news/numTokens_news\n",
        "vocabSize_swbd/numTokens_swbd\n",
        "\n",
        "top10_shakespeare = nltk.FreqDist(tokenized_shakespeare).most_common(10)\n",
        "top10_news = nltk.FreqDist(tokenized_news).most_common(10)\n",
        "top10_swbd = nltk.FreqDist(tokenized_swbd).most_common(10)"
      ],
      "metadata": {
        "id": "J2FhQrRdsJYB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalization**\n",
        "\n",
        "\n",
        "In this next step we will first normalize the tokenized text following these steps (in order):\n",
        "\n",
        "Convert all text to lowercase\n",
        "\n",
        "Remove punctuation (use string.punctuation for a list of punctuation marks)\n",
        "\n",
        "Remove stopwords (use nltk.stopwords for English)"
      ],
      "metadata": {
        "id": "cNbm6IlmsX7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textstat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpRYzbGU9OUH",
        "outputId": "9ab1a836-6c52-44ac-c536-a8b01a6ce936"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textstat in /usr/local/lib/python3.7/dist-packages (0.7.3)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.7/dist-packages (from textstat) (0.13.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string \n",
        "import textstat\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "B5NPNKCTyB-o",
        "outputId": "85a26012-22dc-4cf2-c32b-5fe4f67abce1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lower_shakespeare = [w.lower() for w in tokenized_shakespeare]\n",
        "lower_news = [w.lower() for w in tokenized_news]\n",
        "lower_swbd = [w.lower() for w in tokenized_swbd]\n",
        "\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "noPunctuation_shakespeare = [word for word in lower_shakespeare if word not in punctuation and word != '’']\n",
        "noPunctuation_news = [word for word in lower_news if word not in punctuation and word != '’']\n",
        "noPunctuation_swbd = [word for word in lower_swbd if word not in punctuation and word != '’']\n",
        "\n",
        "stopWords = set(stopwords.words('english'))\n",
        "\n",
        "nostopWords_shakespeare = [word for word in noPunctuation_shakespeare if word not in stopWords]\n",
        "nostopWords_news = [word for word in noPunctuation_news if word not in stopWords]\n",
        "nostopWords_swbd = [word for word in noPunctuation_swbd if word not in stopWords]\n",
        "\n",
        "normalized_shakespeare = nostopWords_shakespeare\n",
        "normalized_news = nostopWords_news\n",
        "normalized_swbd = nostopWords_swbd\n",
        "\n",
        "numTokens_shakespeare = len(normalized_shakespeare)\n",
        "numTokens_news = len(normalized_news)\n",
        "numTokens_swbd = len(normalized_swbd)\n",
        "\n",
        "vocabSize_shakespeare = len(set(normalized_shakespeare))\n",
        "vocabSize_news = len(set(normalized_news))\n",
        "vocabSize_swbd = len(set(normalized_swbd))\n",
        "\n",
        "vocabSize_shakespeare/numTokens_shakespeare\n",
        "vocabSize_news/numTokens_news\n",
        "vocabSize_swbd/numTokens_swbd\n",
        "\n",
        "top10_shakespeare = nltk.FreqDist(normalized_shakespeare).most_common(10)\n",
        "top10_news = nltk.FreqDist(normalized_news).most_common(10)\n",
        "top10_swbd = nltk.FreqDist(normalized_swbd).most_common(10)"
      ],
      "metadata": {
        "id": "Gl2nujsrs-_D"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_data = shakespeare_data[:20000]\n",
        "news_data = news_data[:20000]\n",
        "swbd_data = swbd_data[:20000]\n",
        "\n",
        "#TTR\n",
        "\n",
        "numTokens_shakespeare = len(shakespeare_data)\n",
        "numTokens_news = len(news_data)\n",
        "numTokens_swbd = len(swbd_data)\n",
        "\n",
        "numTypes_shakespeare = len(set(shakespeare_data))\n",
        "numTypes_news = len(set(news_data))\n",
        "numTypes_swbd = len(set(swbd_data))\n",
        "\n",
        "#TTR (Type to Token Ratio)\n",
        "ttr_shakespeare = numTypes_shakespeare/numTokens_shakespeare\n",
        "ttr_news = numTypes_news/numTokens_news\n",
        "ttr_swbd = numTypes_swbd/numTokens_swbd\n",
        "\n",
        "#Flesch Reading Ease Score.\n",
        "#assess the ease of readability in a document\n",
        "#maximum score is 121.22, there is no limit on how low the score can be\n",
        "flesch_shakespeare = textstat.flesch_reading_ease(shakespeare_data)\n",
        "flesch_news = textstat.flesch_reading_ease(news_data)\n",
        "flesch_data = textstat.flesch_reading_ease(swbd_data)\n",
        "\n",
        "#Readability Consensus based upon all the above tests\n",
        "#Based upon all the above tests, returns the estimated school grade level required to understand the text.\n",
        "consensus_shakespeare = textstat.text_standard(shakespeare_data)\n",
        "consensus_news = textstat.text_standard(news_data)\n",
        "consensus_data = textstat.text_standard(swbd_data)\n",
        "\n",
        "# McAlpine EFLAW Readability Score\n",
        "# Returns a score for the readability of an english text for a foreign learner or English, focusing on the number of miniwords and length of sentences.\n",
        "mcalpine_eflaw_shakespeare = textstat.mcalpine_eflaw(shakespeare_data)\n",
        "mcalpine_eflaw_news = textstat.mcalpine_eflaw(news_data)\n",
        "mcalpine_eflaw_data = textstat.mcalpine_eflaw(swbd_data)"
      ],
      "metadata": {
        "id": "OCdDyXlpyV11"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}